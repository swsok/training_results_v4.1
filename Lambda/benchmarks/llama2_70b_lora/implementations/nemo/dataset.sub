#!/bin/bash
#SBATCH --job-name=dataset
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --gpus-per-node=8
#SBATCH --exclusive
#SBATCH --nodelist=calvin-training-node-001,calvin-training-node-002,calvin-training-node-003,calvin-training-node-004    # Specific nodes

echo ${DATAPATH}

_cont_mounts="\
${DATAPATH}:/data,\
${MODELPATH}:/model,\
/dev/infiniband/uverbs0:/dev/infiniband/uverbs0,\
/dev/infiniband/uverbs1:/dev/infiniband/uverbs1,\
/dev/infiniband/uverbs2:/dev/infiniband/uverbs2,\
/dev/infiniband/uverbs3:/dev/infiniband/uverbs3,\
/dev/infiniband/uverbs4:/dev/infiniband/uverbs4,\
/dev/infiniband/uverbs5:/dev/infiniband/uverbs5,\
/dev/infiniband/uverbs6:/dev/infiniband/uverbs6,\
/dev/infiniband/uverbs7:/dev/infiniband/uverbs7"

export NCCL_DEBUG=INFO
export NCCL_P2P_LEVEL=NVL
export NCCL_NET_GDR_LEVEL=1
export NCCL_IB_HCA='=mlx5_1,mlx5_2,mlx5_3,mlx5_4,mlx5_5,mlx5_6,mlx5_7'
export NCCL_IB_PCI_RELAXED_ORDERING=1


export HOME=/home/ubuntu

CONT="local/mlperf-nvidia-llama2_70b_lora:latest"

echo $HEADNODE_HOSTNAME

srun -l --mpi="pmix" \
     --container-image="${HEADNODE_HOSTNAME}:5000#${CONT}" \
     --container-mounts=${_cont_mounts} \
     /bin/bash -c "ulimit -n 65536 && nvidia-smi"

# Data
srun -l --mpi="pmix" \
     --container-image="${HEADNODE_HOSTNAME}:5000#${CONT}" \
     --container-mounts=${_cont_mounts} \
     /bin/bash -c "ulimit -n 65536 && python ./scripts/download_dataset.py && python ./scripts/convert_dataset.py"

# Model
srun -l --mpi="pmix" \
     --container-image="${HEADNODE_HOSTNAME}:5000#${CONT}" \
     --container-mounts=${_cont_mounts} \
     /bin/bash -c "ulimit -n 65536 && python ./scripts/download_model.py --model_dir /model && python ./scripts/convert_model.py --input_name_or_path=/model --output_path=/model/llama2-70b.nemo --hparams_file ./scripts/megatron_llama_config.yaml"

sudo chmod -R 777 $DATAPATH
sudo chmod -R 777 $MODELPATH

# Extract model weights
cd $MODELPATH && tar -xvf llama2-70b.nemo